# Transcript: Model Quantization Performance Comparison

> **Video**: Model Quantization Performance Comparison  
> **URL**: https://www.youtube.com/watch?v=XDzspWgnzxI  
> **Date**: 2026-01-27  
> **Duration**: 18:49  
> **Total Segments**: 35

---

## Full Transcript

[0:00] Greetings everyone. Today I am going to make comparison of the performance for the GGUF Q8, NVFP4, BF16, and FP8 scaled precisions for the Z Image Turbo, FLUX 2 Dev, FLUX 1 Dev, FLUX 1 Kontext Dev models. Moreover, I will compare their quality 1 by 1, side by side, so you will see how much quality degrades or changes between these precisions. Moreover, I will show how you can download them and I will talk about my FP8 quantization application implemented into SECourses Musubi Trainer. And furthermore, my new NVFP4 model quantizer application. To develop this application, I did spend massive amount of time and money. I have used the SimplePod RTX PRO 6000 GPU over a 1 day to make this application work. This was not a trivial task. However, as a result, we got an amazing FLUX SRPO mixed NVFP4 model. You will see how quality it is and how much faster it is.

[1:16] Moreover, FLUX just published a new model called as FLUX 2 Klein 9 billion parameters. Hopefully, I will also cover this model. This model is supposed to be better trainable and since it is much smaller compared to the official FLUX 2, it will work much faster. I hope the quality is amazing so that we can start using this model as well. Hopefully, I will cover it fully with presets, with 1 click downloads. You see the BF16 is only 18 GB. FLUX 2 was 60 GB as a BF16. So this is another model, a tutorial is coming hopefully. So let's begin with the speed differences. To obtain these speeds you need to install ComfyUI CUDA 13 version. Our ComfyUI installer already updated for CUDA 13 version with the latest libraries. I have compiled every one of them myself for you. My compiled libraries, Flash Attention, Sage Attention, xFormers, are compiled for these CUDA archs. Therefore, they are working for every GPU out there that you can think of. If you have watched my latest tutorial, you will learn how to install and use this latest ComfyUI version. The link will be in the description of the video. So let's begin with the speed comparison. The first model is Z Image Turbo and GGUF Q8 speed is 2.26 IT per second. This is for 1536 to 1536 pixel image generation. When we look at the NVFP4 variant precision, it becomes 4.23 IT per second. It is 87 percentage faster compared to the GGUF Q8. The BF16 version is only 10 percentage faster. I saw a significant speed up with the CUDA 13 version for GGUF models so the ComfyUI team is cooking. They are improving the GGUF significantly and you need CUDA 13 version for this one. With the FP8 scaled, it is only 7 percentage faster than the GGUF Q8. So the GGUF is becoming much faster compared to before for Z Image Turbo.

[2:52] What about the quality? For quality comparison, I am going to use image comparison slider. You can download it from here and install. Installation is so simple. All you need to do is install update up .bat file then start image comparison .bat file. The link will be in the description of the video. So I have selected my files. Let's full screen. On the left we see the BF16 version, the highest quality. And on the right now we see the GGUF Q8. You see the GGUF Q8 is almost same as BF16. There isn't visible quality degradation so it is working amazing. When we switch to FP8 scaled, the quality is still very good. We don't see quality degrade, almost same quality. When we look at the NVFP4, we see some quality degrade for the Z Image Turbo. Maybe a better variant, a better version of the NVFP4 will be published. We will see that. Let's look at the FLUX 2 Dev version. With the FLUX 2 Dev version, the GGUF Q8 is very slow, 7.97 second IT. When we look at the NVFP4, it is 100 percentage faster compared to the GGUF Q8. The FP8 scaled version is still significantly faster than the GGUF Q8. Therefore, you should use the FP8 scaled variants for FLUX 2. There is a massive difference. And BF16 version is also slow even though I have did test on RTX 6000 PRO. So for FLUX 2 Dev, I recommend either NVFP4 or FP8 scaled. Let's look at the quality difference. So these are the images. Now on the left we see the BF16 and on the right we see the GGUF Q8. Almost same image, there is no quality degrade. When we look at the mixed FP8 scaled, we see almost same quality. Mixed means that some of the layers of the model is not quantized. So they are BF16 precision in this case. So it is almost same quality, really cool. When we look at the NVFP4, we see some degrade in quality but still very good. Still perfectly usable. Therefore, you can use the NVFP4 variant for the FLUX 2 Dev model. Amazing. Before we move to the FLUX 1 Dev variant, I need to tell you that I have generated with 2048 pixel resolution and Quality 1 preset. Our Quality 1 preset is using heavy sampler, seeds 2. Therefore, it is 2x slow. It also applies to our Z Image Turbo and other presets as well. I am usually using heavy sampler, therefore they are twice slower. So these tests are made with the best quality. So with the FLUX 1 Dev variant, we see GGUF Q8 is 3.54 IT per second. You need to think about the relative speeds, not the exact speeds because exact speeds depends on your resolution, your preset, your GPU. However, relative speeds are valid for same GPU. I have done the tests on SimplePod AI and for every test I did generate multiple images because the initial image is slow and subsequent images will be faster generation. And in the debug menu, I looked the final second IT or IT per second and the duration. This is how I have calculated the durations. So when we look at the NVFP4, it is 118 percent faster than the GGUF Q8. Massive speed difference. BF16 is still 28 percentage faster than the GGUF and FP8 scaled is still 19 percentage faster than GGUF. So for FLUX Dev model, use FP8 scaled or NVFP4 if you are slow, but not GGUF. GGUF is still slow for FLUX Dev. Let's look at the quality of the FLUX Dev model. So the left one is BF16 and the right one is now GGUF. GGUF is almost same as the BF16. But when we look at the FP8 scaled, just a little bit difference, still very good quality. Can be perfectly used. However, when we look at the NVFP4, we see some degrade in quality. It is a noticeable quality degrade. So it is up to you to use or not, you can test it. So let's also look at the FLUX Kontext Dev model speed differences and quality differences. This is editing model. If you don't know how to use this model or what it does, I have an excellent tutorial for it. The link will be in the description of the video. This tutorial, when you open this tutorial and when you look at the video chapters, you will see how to do outpainting, how to use FLUX Kontext to fix images and more information. This is a really good tutorial. So I recommend you to watch this tutorial as well if you don't know how to use this FLUX Kontext Dev model. So we see that GGUF Q8 speed is 1.83 IT per second. The NVFP4 is 93 percentage faster than GGUF Q8. BF16 is 14 percentage faster than GGUF and FP8 scaled is almost same as GGUF, only 9 percentage faster. So you can use GGUF or FP8 scaled for this model. NVFP4 is very fast. But what about the quality difference? So let's select the files. This is how you select multiple files. And let's full screen. Okay, on the left we see the original image and on the right we see the Kontext Edit image. So I changed my hair, I made it longer. This was the prompt. You see the face is almost not changed, very good. Only the hair is changed. This is the test case. So let's select BF16 and GGUF Q8. We almost don't see any difference. It is almost unnoticeable. When we look at the FP8 scaled, still very good quality, almost no noticeable difference. And this FP8 quant scaled is a model that I have generated myself. It is in our downloader application. If you don't know how to use our downloader application, it is so simple. You download the latest SwarmUI model downloader SwarmUI installer from this post. Extract it into your SwarmUI installation folder. Then just double click Windows start download models up .bat file. Then you will get to this screen. You can give your custom model paths, you can download anywhere you want. We have image bundles. You see NVFP4 images bundle, Z Image Turbo models core bundle, FLUX models core bundle. You can download every file individually or as a bundle. One another thing is that downloading models on cloud machines are much harder than downloading and using on our computer. Therefore, our unified model downloader also supports URL downloader so that you can download models from CivitAI, from Hugging Face or any other platform. Just paste the link here and select the folder wherever you want to download. Then it will download it with maximum speed with hash calculation, hash verification. For example, as a demo, let's download this model. So I will right click and copy link address of this model and paste it here and I will download it into here. Then I will click download. It will start the download with maximum speed of my internet connection. You see it is downloading with 16 connection. Therefore, I am reaching 100 megabytes per second on my personal Windows computer. On cloud you can reach to 1 gigabytes per second. This download tool is extremely useful if you want to use it. You can search for the files from here like "Kontext" and it will list me all the files. You see FLUX Kontext Dev quantized model. And to make this quantization we are using SECourses Musubi Trainer application. The link will be in the description of the video. And we have a full tutorial of how to use SECourses Musubi Trainer. It is in this tutorial so you can also watch this or Wan 2.2 training tutorial. Either of them works. So this application has FP8 model converter. Normally we were using Musubi style but I have recently added quant version. This is much more advanced. It is using specific quantization. This is how I generated this amazing FLUX Kontext Dev Quant FP8 scaled. I used that also to generate FLUX Dev Quant FP8 scaled. These are myself generated models. They are available in our downloader. So we don't see any quality difference. It is amazing quality, almost as BF16. And when we look at the NVFP4, NVFP4 also did amazing job. It is still very good quality so perfectly usable. And finally, the new FLUX SRPO model NVFP4 mixed precision. To make this model I spent literally 2 days, a lot of money because quantization is only possible with 48 GB GPUs. Therefore I had to use PRO 6000 on SimplePod. So you see there is almost no quality difference. They are both amazing and it is extremely fast. Let's make a live demonstration. This is running locally, my local SwarmUI. And let me show you my nvitop as well. So let's generate 8 images with the FLUX SRPO mixed NVFP4. You will be shocked by the speed of it. So it started generation. Okay it is generating. You are watching it live. It takes like 5 seconds for 40 steps and the highest quality. You see the quality is amazing. It is working amazing. It is taking only 5.7 seconds to generate on RTX 5090. And how much memory it is using? It is using 14 GB of VRAM memory. It would use lesser memory if I had a lower end GPU like 8 GB GPU. However, it is working amazing. By the way, NVFP4 models bringing speed only on RTX 5000 series but you can use it on other GPUs as well. Other models like 4000, 3000 series. So it is amazing. When we compare it with the BF16, let's look at the BF16 difference. So here. It will take more than twice time. It will take like 12 seconds. So now it is loading the model and it will also use much more VRAM. So if you have 5000 series GPUs, like 5060 or 5070, 5080, this will work amazing. Okay it is loading the model. Yes, it is using 26 GB of VRAM. As I said numerous times, don't worry about your VRAM. Even if you have 8 GB GPU or 6 GB GPU, it will still work as long as you have RAM memory. Because ComfyUI is automatically doing all the block swapping, VRAM streaming for you. So it will still work very fast and it will work on low end GPUs. As long as you have RAM memory don't worry about it. So it is taking like 14 seconds. It is more than twice. This is an amazing speed difference. However, recently since ComfyUI is doing a lot of updates, you may be needed to add some arguments to your ComfyUI or SwarmUI backend. Which are them? If you are on low RAM memory, RAM memory not VRAM, you can use cache none. If you use cache none, it will not keep any model on the RAM or VRAM. It will deload them, even VAE or text encoder. So this will be minimal RAM and VRAM usage. Or you can add this disable smart memory and it will return back to older VRAM management. How you do that? You add it into here and it will use it. Or in ComfyUI, as I have shown in our previous tutorials, you edit this run GPU .bat file like this and you add that like this. So it will use that argument. This is how you add arguments to your ComfyUI installation or SwarmUI installation that uses the our ComfyUI installation. And they will fix your out of VRAM errors or stuck or freeze errors. Because ComfyUI is updating, they are fixing, sometimes they are breaking. So you can use these arguments to fix the issues. But I can say that you should definitely upgrade to ComfyUI to the CUDA 13. Why? Because it is faster overall and especially faster for GGUF models and it will be from now on developed better and better. Therefore, it is recommended from now on. Our installers are all up to date, updated. We have all the presets. I also have converted these presets into ComfyUI so you can use that. Moreover, if you are remembering in our latest tutorial, we had introduced you the SimplePod AI. I have updated all the zip files. So you can just run "run pod SimplePod comfyui instructions". You will have all the links. So please use these links to register. Use this link as a template. And the team fixed the errors. So when you selected this template, edit and use, deploy your persistent volume, set your mount storage as workspace. You can also use without persistent storage then it will be ready to use. Select any GPU you want. And if you had watched the previous tutorial, you remember that the RTX 6000 Blackwell GPUs were not working. They fixed that issue. I did run this test on the RTX PRO 6000 GPUs and it is working perfect with 100 percentage utilization. Let me demonstrate you. For example, this is running on SimplePod AI. Let's continue. Open a terminal. Let's pip install nvitop. And nvitop. We see that 600 watts is being used so it is fully utilizing. Moreover, they have added also extract option to the Jupyter Lab interface. So now you can extract zip files as well. You see when I right click, extract archive is there. So it is also fixed and working. And you can see that how much time and tests I have done. Let me show you. You see all these mixed precisions NVFP4 tests. All of them were failing. I did a lot of, a lot of testing and fixing. So this NVFP4 converter application was really hard to program. I did spend huge money and time on it. All of the links will be in the description of the video. Like any other of my videos, you see I put the links like this. So this is how you will find the links. You can contact me by replying to my videos or by joining our Discord server. The link is here. You can also message me from LinkedIn. You can also send me an email. Everything is fine. I reply all of them. Discord is the best way to contact me or Patreon or replying to the YouTube. Hopefully see you in another amazing tutorial video.

---

## Timestamped Segments

- `[0:00]` - Introduction: Comparison of GGUF Q8, NVFP4, BF16, FP8 precisions for Z-Image-Turbo, FLUX models
- `[0:38]` - FP8 quantization application and NVFP4 model quantizer development
- `[1:16]` - FLUX 2 Klein announcement (9B parameters, 18GB BF16)
- `[1:47]` - Speed comparison begins: Z-Image-Turbo results
- `[2:52]` - Quality comparison methodology using image comparison slider
- `[4:35]` - FLUX 2 Dev speed and quality results
- `[5:45]` - FLUX 1 Dev speed and quality results (2048px, Quality 1 preset)
- `[8:00]` - FLUX Kontext Dev (editing model) speed and quality results
- `[10:03]` - Unified model downloader tool demonstration
- `[11:39]` - SECourses Musubi Trainer FP8 quantization tool
- `[12:44]` - FLUX SRPO Mixed NVFP4 live demonstration (5.7s generation, 14GB VRAM)
- `[15:03]` - ComfyUI memory management arguments (--cache none, --disable-smart-memory)
- `[16:40]` - CUDA 13 ComfyUI upgrade recommendation
- `[17:11]` - SimplePod AI updates and RTX 6000 Blackwell GPU fix
- `[18:18]` - NVFP4 converter development challenges and testing
