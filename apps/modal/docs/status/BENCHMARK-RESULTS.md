# Modal Endpoint Benchmark Report

**Generated:** 2026-02-01 19:42:36 UTC  
**Workspace:** ryla  
**Test Resources:** Image=✅, Video=❌, LoRA=❌

## Summary

| Metric | Value |
|--------|-------|
| Total Endpoints | 3 |
| Successful | 3 |
| Failed | 0 |
| Skipped | 0 |
| Total Benchmark Cost | $0.0000 |

## Results by Category

### Qwen Image

| Endpoint | Path | Cold (s) | Warm (s) | Δ Savings | Cost ($) | Status |
|----------|------|----------|----------|-----------|----------|--------|
| Qwen Image 2512 Fast | `/qwen-image-2512-fast` | 64.8 | 2.3 | 96% | - | ✅ |

### Z-Image

| Endpoint | Path | Cold (s) | Warm (s) | Δ Savings | Cost ($) | Status |
|----------|------|----------|----------|-----------|----------|--------|
| Z Image Simple | `/z-image-simple` | 53.5 | 2.0 | 96% | - | ✅ |
| Z Image Danrisi | `/z-image-danrisi` | - | 2.1 | - | - | ✅ |

## Key Findings

- **Average Cold Start:** 59.1s
- **Average Warm Time:** 2.1s  
- **Warm vs Cold Improvement:** 96% faster
- **Total Benchmark Cost:** $0.0000

## Recommendations

1. **Keep containers warm** for production - warm containers are significantly faster
2. **Use Z-Image Danrisi** for fastest image generation (minimal cold start overhead)
3. **Batch video requests** - video endpoints have the longest cold starts
4. **Pre-warm before peak hours** - schedule periodic pings to keep containers warm

## How to Run This Benchmark

```bash
# Full benchmark (cold + warm)
python apps/modal/scripts/benchmark-endpoints.py

# Quick benchmark (warm only)
python apps/modal/scripts/benchmark-endpoints.py --quick

# Generate test resources only
python apps/modal/scripts/benchmark-endpoints.py --generate

# To enable LoRA endpoints, train a test LoRA first:
modal run apps/modal/apps/lora-training/app.py --character-id=test-benchmark --steps=100
```

---
*Report generated by `benchmark-endpoints.py`*
